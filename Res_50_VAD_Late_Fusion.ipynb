{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529727b2-65fa-494e-9478-9090efd237a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "import matplotlib.patches as mpatches\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "import torchvision.models as models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c73a0d4-c3fb-4434-bd96-624e6bde7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, gt_masks_path=None, num_sequences=None, is_training=True, transform=None, target_size=(224, 144), segment_length=17):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.gt_masks_path = gt_masks_path\n",
    "        self.num_sequences = num_sequences\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.segment_length = segment_length\n",
    "        self.max_track_id = 19  \n",
    "        self.track_id_mapping = self.create_track_id_mapping()\n",
    "        self.sequence_folders = sorted(os.listdir(self.dataset_path))\n",
    "        # Filter out the ground truth folders and unwanted folders\n",
    "        self.sequence_folders = sorted([\n",
    "            folder for folder in os.listdir(self.dataset_path) \n",
    "            if os.path.isdir(os.path.join(self.dataset_path, folder)) \n",
    "            and '_gt' not in folder \n",
    "            and '.ipynb_checkpoints' not in folder\n",
    "        ])\n",
    "    def create_track_id_mapping(self):\n",
    "        track_ids = np.arange(1, self.max_track_id + 1)\n",
    "        normalized_track_ids = track_ids / self.max_track_id\n",
    "        track_id_mapping = dict(zip(track_ids, normalized_track_ids))\n",
    "        return track_id_mapping\n",
    "\n",
    "    def create_segments(self, frames, labels, segment_length=11):\n",
    "        num_frames = len(frames)\n",
    "        half_segment = segment_length // 2\n",
    "\n",
    "        segments = []\n",
    "        segment_labels = []\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            segment = []\n",
    "\n",
    "            for j in range(i - half_segment, i):\n",
    "                if j < 0:\n",
    "                    segment.append(np.zeros_like(frames[0])) \n",
    "                else:\n",
    "                    segment.append(frames[j])\n",
    "\n",
    "            segment.append(frames[i])\n",
    "\n",
    "            for j in range(i + 1, i + 1 + half_segment):\n",
    "                if j >= num_frames:\n",
    "                    segment.append(np.zeros_like(frames[0])) \n",
    "                else:\n",
    "                    segment.append(frames[j])\n",
    "           \n",
    "            segments.append(segment)\n",
    "            segment_labels.append(labels[i])     \n",
    "        \n",
    "        return segments, segment_labels\n",
    "    \n",
    "    def process_sequence(self, sequence_folder):        \n",
    "        sequence_path = os.path.join(self.dataset_path, sequence_folder)\n",
    "        # Check if the sequence folder is a directory and doesn't contain '_gt' or '.ipynb_checkpoints'\n",
    "        if not os.path.isdir(sequence_path):\n",
    "            return [], [], []\n",
    "    \n",
    "        \n",
    "        sequence_frames = []\n",
    "        sequence_tracking_data = []\n",
    "        sequence_labels = []\n",
    "        \n",
    "        label_folder = os.path.join(sequence_path, 'labels')\n",
    "        if not os.path.exists(label_folder):\n",
    "            return [], [], []\n",
    "\n",
    "        tracking_data = {}\n",
    "\n",
    "        for label_file in sorted(os.listdir(label_folder)):\n",
    "            if not label_file.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            frame_id = int(os.path.splitext(label_file)[0])\n",
    "            tracking_data[frame_id] = []\n",
    "\n",
    "            with open(os.path.join(label_folder, label_file), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line_split = line.strip().split(' ')\n",
    "                    if len(line_split) < 6:\n",
    "                        continue\n",
    "\n",
    "                    x = float(line_split[1])\n",
    "                    y = float(line_split[2])\n",
    "                    width = float(line_split[3])\n",
    "                    height = float(line_split[4])\n",
    "                    track_id = int(line_split[5])\n",
    "                    tracking_data[frame_id].append({'track_id': track_id, 'x': x, 'y': y, 'width': width, 'height': height})\n",
    "\n",
    "        for frame_index, track_data_list in tracking_data.items():\n",
    "            img_filename = \"{:03d}.tif\".format(frame_index)\n",
    "            img_path = os.path.join(sequence_path, img_filename)\n",
    "            frame = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            #frame = cv2.resize(frame, self.target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            tracking_data_channel = np.zeros_like(frame, dtype=np.float32)                \n",
    "\n",
    "            for track_data in track_data_list:\n",
    "                x, y = track_data['x'], track_data['y']\n",
    "                width, height = track_data['width'], track_data['height']\n",
    "                track_id = track_data['track_id']\n",
    "                normalized_track_id = self.track_id_mapping.get(track_id, 0.0)\n",
    "\n",
    "                x_pixel = int(x * frame.shape[1])\n",
    "                y_pixel = int(y * frame.shape[0])\n",
    "                width_pixel = int(width * frame.shape[1])\n",
    "                height_pixel = int(height * frame.shape[0])\n",
    "\n",
    "                for y_idx in range(y_pixel, y_pixel + height_pixel):\n",
    "                    for x_idx in range(x_pixel, x_pixel + width_pixel):\n",
    "                        if 0 <= x_idx < frame.shape[1] and 0 <= y_idx < frame.shape[0]:\n",
    "                            tracking_data_channel[y_idx, x_idx] = normalized_track_id                                \n",
    "\n",
    "            frame = np.expand_dims(frame, axis=-1)            \n",
    "            tracking_data_channel = np.expand_dims(tracking_data_channel, axis=-1)\n",
    "            # Convert numpy arrays to PIL Images\n",
    "            frame_pil = Image.fromarray(frame.squeeze(axis=-1))\n",
    "            tracking_data_channel_pil = Image.fromarray(tracking_data_channel.squeeze(axis=-1))\n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                seed = np.random.randint(0, 2**32)  # Generate random seed\n",
    "                random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "\n",
    "                frame_pil = self.transform(frame_pil)\n",
    "                random.seed(seed)  # Reset the seed\n",
    "                torch.manual_seed(seed)\n",
    "\n",
    "                tracking_data_channel_pil = self.transform(tracking_data_channel_pil)\n",
    "            # Convert the transformed PIL images back to numpy arrays\n",
    "            frame = np.array(frame_pil)\n",
    "            \n",
    "            tracking_data_channel = np.array(tracking_data_channel_pil)\n",
    "            frame = np.expand_dims(frame, axis=-1)   \n",
    "            tracking_data_channel = np.expand_dims(tracking_data_channel, axis=-1)               \n",
    "            sequence_frames.append(frame)           \n",
    "            sequence_tracking_data.append(tracking_data_channel)\n",
    "\n",
    "            if not self.is_training and self.gt_masks_path:\n",
    "                gt_mask_path = os.path.join(self.gt_masks_path, sequence_folder + \"_gt\", img_filename.replace('.tif', '.bmp'))\n",
    "                if os.path.exists(gt_mask_path):\n",
    "                    gt_mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if gt_mask is not None:\n",
    "                        label = int(np.max(gt_mask) > 0)\n",
    "                        sequence_labels.append(label)\n",
    "                    else:\n",
    "                        label = 0\n",
    "                        sequence_labels.append(label)\n",
    "                else:\n",
    "                    label = 0\n",
    "                    sequence_labels.append(label)\n",
    "            elif self.is_training:\n",
    "                label = 0\n",
    "                sequence_labels.append(label)\n",
    "\n",
    "        segment_frames, segment_labels = self.create_segments(sequence_frames, sequence_labels, segment_length=self.segment_length)\n",
    "        segment_tracking_data, _ = self.create_segments(sequence_tracking_data, segment_labels, segment_length=self.segment_length)\n",
    "\n",
    "        segment_frames = np.stack(segment_frames)\n",
    "        segment_tracking_data = np.stack(segment_tracking_data)\n",
    "        segment_labels = np.stack(segment_labels)\n",
    "        return segment_frames, segment_tracking_data, segment_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_folder = self.sequence_folders[idx]\n",
    "        segment_frames, segment_tracking_data, segment_labels = self.process_sequence(sequence_folder)\n",
    "        return segment_frames, segment_tracking_data, segment_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6095d084-b351-4516-9744-35e5fb44ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path and ground truth masks path\n",
    "train_dataset_path = '/mmfs1/scratch/jacks.local/ojanigala/Anomaly_Detection/UCSDped1/Train'  # Path to the train dataset\n",
    "test_dataset_path = '/mmfs1/scratch/jacks.local/ojanigala/Anomaly_Detection/UCSDped1/Test'    # Path to the test dataset\n",
    "test_gt_masks_path = '/mmfs1/scratch/jacks.local/ojanigala/Anomaly_Detection/UCSDped1/Test'   # Path to the test ground truth masks\n",
    "\n",
    "# Define transformations for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "    # Initialize the dataset for training and testing\n",
    "train_dataset = CustomDataset(train_dataset_path, is_training=True)\n",
    "test_dataset = CustomDataset(test_dataset_path, gt_masks_path=test_gt_masks_path, is_training=False)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(\"Error loading datasets:\", e)\n",
    "#     exit(1)\n",
    "\n",
    "\n",
    "# Calculate the split index\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(train_dataset)))\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02598263-8b46-44e4-ab1c-d877614b272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train subset: 27\n",
      "Number of samples in the val subset: 7\n",
      "Number of samples in the test: 36\n",
      "Batch Frames shape: torch.Size([1, 200, 17, 158, 238, 1])\n",
      "Batch Tracking Data shape: torch.Size([1, 200, 17, 158, 238, 1])\n",
      "Batch Labels shape: torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# Set a unique seed value for this notebook\n",
    "SEED = 42  \n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure deterministic behavior for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "full_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "train_subset_length = len(train_subset)\n",
    "val_subset_length = len(val_subset)\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "print(f\"Number of samples in the train subset: {train_subset_length}\")\n",
    "print(f\"Number of samples in the val subset: {val_subset_length}\")\n",
    "print(f\"Number of samples in the test: {test_length}\")\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    frames, tracking_data, labels = batch\n",
    "    print(\"Batch Frames shape:\", frames.shape)\n",
    "    print(\"Batch Tracking Data shape:\", tracking_data.shape)\n",
    "    print(\"Batch Labels shape:\", labels.shape)\n",
    "    if i == 0: \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a29985c-1fe6-4451-ab95-472394cfe7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "        \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        # print(\"BasicBlock Module...\")\n",
    "        # print_memory_usage()\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv3 = nn.Conv3d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)  \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)  \n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        # print(\"Bottleneck Module...\")\n",
    "        # print_memory_usage()\n",
    "        return out\n",
    "\n",
    "        \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool3d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels // reduction_ratio, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels // reduction_ratio, in_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv3d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm3d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        out = out * self.spatial_attention(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, height, width, frame_channel, track_channel, output_channels=512):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.frame_channel = frame_channel\n",
    "        self.track_channel = track_channel\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        # Calculate the flattened input size based on image dimensions\n",
    "        frame_flattened_size = self.frame_channel * self.height * self.width  # 1 * 158 * 238 = 37564\n",
    "        tracking_flattened_size = self.track_channel * self.height * self.width  # 1 * 158 * 238 = 37564\n",
    "\n",
    "        # Adjust the linear layers to match the correct flattened size\n",
    "        self.frame_embed = nn.Linear(frame_flattened_size, embed_dim)\n",
    "        self.tracking_embed = nn.Linear(tracking_flattened_size, embed_dim)\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        self.output_conv = nn.Conv2d(embed_dim, output_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, frames, tracking_data):\n",
    "        batch_size, num_channels_frames, num_frames, height, width = frames.shape\n",
    "        _, num_channels_tracking, num_tracking_frames, _, _ = tracking_data.shape\n",
    "    \n",
    "        # Flatten each frame and tracking data to be used as tokens\n",
    "        frames_flat = frames.reshape(batch_size * num_frames, -1)  # Flatten to (batch_size * num_frames, flattened_size)\n",
    "        tracking_data_flat = tracking_data.reshape(batch_size * num_tracking_frames, -1)  # Same for tracking data\n",
    "    \n",
    "        # print(f\"Flattened frames shape: {frames_flat.shape}\")  # Debugging line\n",
    "        # print(f\"Flattened tracking data shape: {tracking_data_flat.shape}\")  # Debugging line\n",
    "\n",
    "        # Embed tokens\n",
    "        frames_embedded = self.frame_embed(frames_flat)  # Shape: (batch_size * num_frames, embed_dim)\n",
    "        tracking_embedded = self.tracking_embed(tracking_data_flat)  # Shape: (batch_size * num_tracking_frames, embed_dim)\n",
    "    \n",
    "        # Create query, key, and value for cross-attention\n",
    "        query = frames_embedded\n",
    "        key = tracking_embedded\n",
    "        value = frames_embedded\n",
    "        \n",
    "        attn_output, attn_weights = self.cross_attention(query, key, value)\n",
    "        \n",
    "        attn_output = attn_output.transpose(0, 1)  # Shape: (batch_size, num_frames, embed_dim)\n",
    "        \n",
    "        # Reshape for convolutional layer\n",
    "        attn_output = attn_output.reshape(batch_size, self.embed_dim, num_frames, 1)  # Shape: (batch_size, embed_dim, num_frames, 1)\n",
    "        \n",
    "        attn_output = self.output_conv(attn_output)  # Shape: (batch_size, output_channels, num_frames, 1)\n",
    "        \n",
    "        # Reshape to match the original input dimensions (batch_size, output_channels, num_frames, height, width)\n",
    "        attn_output = attn_output.view(batch_size, self.output_channels, num_frames, 1, 1)  # Shape: (batch_size, output_channels, num_frames, 1, 1)\n",
    "        attn_output = attn_output.expand(-1, -1, -1, self.height, self.width)  # Expand to (batch_size, output_channels, num_frames, height, width)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1, in_channels=1):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout3d(p=0.5)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResNet3D_CBAM(ResNet3D):\n",
    "    def __init__(self, block, layers, num_classes=1, in_channels=1):\n",
    "        super(ResNet3D_CBAM, self).__init__(block, layers, num_classes, in_channels)\n",
    "        \n",
    "        # CBAM modules for the ResNet3D layers\n",
    "        self.cbam1 = CBAM(64 * block.expansion)\n",
    "        self.cbam2 = CBAM(128 * block.expansion)\n",
    "        self.cbam3 = CBAM(256 * block.expansion)\n",
    "        self.cbam4 = CBAM(512 * block.expansion)\n",
    "\n",
    "        # Cross-Attention for late fusion\n",
    "        self.embed_dim = 2048  # Match this to the ResNet output dimension\n",
    "        self.num_heads = 4\n",
    "        self.cross_attention = CrossAttentionFusion(embed_dim=self.embed_dim, num_heads=self.num_heads,\n",
    "                                                    height=5, width=8, \n",
    "                                                    frame_channel=2048, track_channel=2048, output_channels=2048).to(device)\n",
    "\n",
    "        # Final classification layer after fusion\n",
    "        self.fc_final = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x_frames, x_track):\n",
    "        # Process frames modality\n",
    "        x_frames = self.conv1(x_frames)\n",
    "        x_frames = self.bn1(x_frames)\n",
    "        x_frames = self.relu(x_frames)\n",
    "        x_frames = self.maxpool(x_frames)\n",
    "\n",
    "        x_frames = self.layer1(x_frames)\n",
    "        x_frames = self.cbam1(x_frames)\n",
    "        x_frames = self.layer2(x_frames)\n",
    "        x_frames = self.cbam2(x_frames)\n",
    "        x_frames = self.layer3(x_frames)\n",
    "        x_frames = self.cbam3(x_frames)\n",
    "        x_frames = self.layer4(x_frames)\n",
    "        x_frames = self.cbam4(x_frames)\n",
    "        #print(\"x_frames:\",x_frames.shape)\n",
    "        # Process tracking data modality (same ResNet3D processing)\n",
    "        x_track = self.conv1(x_track)\n",
    "        x_track = self.bn1(x_track)\n",
    "        x_track = self.relu(x_track)\n",
    "        x_track = self.maxpool(x_track)\n",
    "\n",
    "        x_track = self.layer1(x_track)\n",
    "        x_track = self.cbam1(x_track)\n",
    "        x_track = self.layer2(x_track)\n",
    "        x_track = self.cbam2(x_track)\n",
    "        x_track = self.layer3(x_track)\n",
    "        x_track = self.cbam3(x_track)\n",
    "        x_track = self.layer4(x_track)\n",
    "        x_track = self.cbam4(x_track)\n",
    "        #print(\"x_track:\",x_track.shape)\n",
    "        # Apply cross-attention to fuse the modalities\n",
    "        x_fused = self.cross_attention(x_frames, x_track)\n",
    "\n",
    "        # Perform pooling and classification on the fused output\n",
    "        x_fused = self.avgpool(x_fused)\n",
    "        x_fused = x_fused.view(x_fused.size(0), -1)\n",
    "        #print(\"x_fused:\",x_fused.shape)\n",
    "        # Final classification layer\n",
    "        x_out = self.fc_final(x_fused)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resnet10_cbam(in_channels=1):\n",
    "    return ResNet3D_CBAM(BasicBlock, [1, 1, 1, 1], in_channels=in_channels)\n",
    "\n",
    "def resnet18_cbam(in_channels=1):\n",
    "    return ResNet3D_CBAM(BasicBlock, [2, 2, 2, 2], in_channels=in_channels)\n",
    "\n",
    "def resnet34_cbam(in_channels=1):\n",
    "    return ResNet3D_CBAM(BasicBlock, [3, 4, 6, 3], in_channels=in_channels)\n",
    "\n",
    "def resnet50_cbam(in_channels=1):\n",
    "    return ResNet3D_CBAM(Bottleneck, [3, 4, 6, 3], in_channels=in_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf70f7-43a0-41f8-bdd2-9807e7f7c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Epoch 1/200, Train Loss: 0.474296, Train Accuracy: 99.15%, Train Precision: 1.0000, Train Recall: 1.0000, Train F1: 1.0000\n",
      "Epoch 1/200, Val Loss: 0.304804, Val Accuracy: 100.00%, Val Precision: 1.0000, Val Recall: 1.0000, Val F1: 1.0000\n",
      "Epoch 2/200, Train Loss: 0.049136, Train Accuracy: 100.00%, Train Precision: 1.0000, Train Recall: 1.0000, Train F1: 1.0000\n",
      "Epoch 2/200, Val Loss: 0.019406, Val Accuracy: 100.00%, Val Precision: 1.0000, Val Recall: 1.0000, Val F1: 1.0000\n",
      "Epoch 3/200, Train Loss: 0.001957, Train Accuracy: 100.00%, Train Precision: 1.0000, Train Recall: 1.0000, Train F1: 1.0000\n",
      "Epoch 3/200, Val Loss: 0.008578, Val Accuracy: 100.00%, Val Precision: 1.0000, Val Recall: 1.0000, Val F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define model, criterion, optimizer, and scheduler\n",
    "model = resnet50_cbam(in_channels=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.000002, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Use DataParallel to utilize multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Training model with Early Stopping \n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_aucs = []\n",
    "    val_aucs = []\n",
    "    train_precisions = []\n",
    "    val_precisions = []\n",
    "    train_recalls = []\n",
    "    val_recalls = []\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "        \n",
    "        for frames, tracking_data, labels in train_loader:\n",
    "            frames, tracking_data, labels = frames.to(device), tracking_data.to(device), labels.to(device)\n",
    "            frames, tracking_data, labels = frames.float(), tracking_data.float(), labels.float()\n",
    "            \n",
    "            # Flatten the first two dimensions (batch, frames)\n",
    "            frames = frames.view(-1, frames.shape[1], frames.shape[2], frames.shape[3], frames.shape[4])\n",
    "            #print(frames.shape)\n",
    "            frames= frames.permute(1, 0, 2, 3, 4)  \n",
    "            #print(frames.shape)\n",
    "            tracking_data = tracking_data.view(-1, tracking_data.shape[1], tracking_data.shape[2], tracking_data.shape[3],tracking_data.shape[4])\n",
    "            tracking_data = tracking_data.permute(1, 0, 2, 3, 4)          \n",
    "           \n",
    "            labels = labels.view(-1)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames, tracking_data)\n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_train = torch.sigmoid(outputs.view(-1)) > 0.5\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Collect labels and predictions for AUC and other metrics\n",
    "            all_train_labels.append(labels.cpu().numpy())\n",
    "            all_train_preds.append(outputs.view(-1).cpu().detach().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        #train_auc = roc_auc_score(np.concatenate(all_train_labels), np.concatenate(all_train_preds))\n",
    "        train_precision = precision_score(np.concatenate(all_train_labels), np.concatenate(all_train_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        train_recall = recall_score(np.concatenate(all_train_labels), np.concatenate(all_train_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        train_f1 = f1_score(np.concatenate(all_train_labels), np.concatenate(all_train_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        #train_aucs.append(train_auc)\n",
    "        train_precisions.append(train_precision)\n",
    "        train_recalls.append(train_recall)\n",
    "        train_f1s.append(train_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train Accuracy: {train_accuracy:.2f}%, Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for frames, tracking_data, labels in val_loader:\n",
    "                frames, tracking_data, labels = frames.to(device), tracking_data.to(device), labels.to(device)\n",
    "                frames, tracking_data, labels = frames.float(), tracking_data.float(), labels.float()\n",
    "                \n",
    "                # Flatten the first two dimensions (batch, frames)\n",
    "                frames = frames.view(-1, frames.shape[1], frames.shape[2], frames.shape[3], frames.shape[4])\n",
    "                frames= frames.permute(1, 0, 2, 3, 4)            \n",
    "                tracking_data = tracking_data.view(-1, tracking_data.shape[1], tracking_data.shape[2], tracking_data.shape[3],tracking_data.shape[4])\n",
    "                tracking_data = tracking_data.permute(1, 0, 2, 3, 4)    \n",
    "                labels = labels.view(-1)\n",
    "                \n",
    "                outputs = model(frames, tracking_data)\n",
    "                loss = criterion(outputs.view(-1), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted_val = torch.sigmoid(outputs.view(-1)) > 0.5\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted_val == labels).sum().item()\n",
    "                \n",
    "                # Collect labels and predictions for AUC and other metrics\n",
    "                all_val_labels.append(labels.cpu().numpy())\n",
    "                all_val_preds.append(outputs.view(-1).cpu().detach().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        #val_auc = roc_auc_score(np.concatenate(all_val_labels), np.concatenate(all_val_preds))\n",
    "        val_precision = precision_score(np.concatenate(all_val_labels),np.concatenate(all_val_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        val_recall = recall_score(np.concatenate(all_val_labels), np.concatenate(all_val_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        val_f1 = f1_score(np.concatenate(all_val_labels), np.concatenate(all_val_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        #val_aucs.append(val_auc)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss:.6f}, Val Accuracy: {val_accuracy:.2f}%, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(best_model_state, 'best_model.pth')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s\n",
    "\n",
    "# Timing training / validation\n",
    "start_time = time.time()\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=200, patience=5)\n",
    "end_time = time.time()\n",
    "train_val_duration = end_time - start_time\n",
    "print(f\"Training / Valdiation Time: {train_val_duration:.2f} seconds\")\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(np.arange(1, len(train_losses) + 1), train_losses, label='Training Loss', linestyle='-')\n",
    "plt.plot(np.arange(1, len(val_losses) + 1), val_losses, label='Validation Loss', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(np.arange(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy', linestyle='-')\n",
    "plt.plot(np.arange(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "# Plot training and validation Precision\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(np.arange(1, len(train_precisions) + 1), train_precisions, label='Training Precision', linestyle='-')\n",
    "plt.plot(np.arange(1, len(val_precisions) + 1), val_precisions, label='Validation Precision', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Training and Validation Precision per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation Recall\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(np.arange(1, len(train_recalls) + 1), train_recalls, label='Training Recall', linestyle='-')\n",
    "plt.plot(np.arange(1, len(val_recalls) + 1), val_recalls, label='Validation Recall', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training and Validation Recall per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Plot training and validation F1 Score\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(np.arange(1, len(train_f1s) + 1), train_f1s, label='Training F1 Score', linestyle='-')\n",
    "plt.plot(np.arange(1, len(val_f1s) + 1), val_f1s, label='Validation F1 Score', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training and Validation F1 Score per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c9a40-adfa-4504-afc6-6a9548e362f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average Train Loss: {np.mean(train_losses)}')\n",
    "print(f'Average Val Loss: {np.mean(val_losses)}')\n",
    "print(f'Average Train Accuracy: {np.mean(train_accuracies)}')\n",
    "print(f'Average Val Accuracy: {np.mean(val_accuracies)}') \n",
    "print(f'Average Train Accuracy: {np.mean(train_precisions)}')\n",
    "print(f'Average Val Accuracy: {np.mean(val_precisions)}') \n",
    "print(f'Average Train Accuracy: {np.mean(train_recalls)}')\n",
    "print(f'Average Val Accuracy: {np.mean(val_recalls)}') \n",
    "print(f'Average Train Accuracy: {np.mean(train_f1s)}')\n",
    "print(f'Average Val Accuracy: {np.mean(val_f1s)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a31df-43bb-4b6a-859a-fd0102a297cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, criterion, optimizer, and scheduler\n",
    "model = resnet50_cbam(in_channels=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.000002, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Use DataParallel to utilize multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "def retrain_model(model, full_train_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
    "    retrain_losses = []\n",
    "    retrain_accuracies = []\n",
    "    retrain_aucs = []    \n",
    "    retrain_precisions = []    \n",
    "    retrain_recalls = []   \n",
    "    retrain_f1s = []    \n",
    "    best_retrain_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        retrain_loss = 0.0\n",
    "        correct_retrain = 0\n",
    "        total_retrain = 0\n",
    "        all_retrain_labels = []\n",
    "        all_retrain_preds = []\n",
    "        \n",
    "        for frames, tracking_data, labels in full_train_loader:\n",
    "            frames, tracking_data, labels = frames.to(device), tracking_data.to(device), labels.to(device)\n",
    "            frames, tracking_data, labels = frames.float(), tracking_data.float(), labels.float()\n",
    "            \n",
    "            # Flatten the first two dimensions (batch, frames)\n",
    "            frames = frames.view(-1, frames.shape[1], frames.shape[2], frames.shape[3], frames.shape[4])\n",
    "            frames= frames.permute(1, 0, 2, 3, 4)            \n",
    "            tracking_data = tracking_data.view(-1, tracking_data.shape[1], tracking_data.shape[2], tracking_data.shape[3],tracking_data.shape[4])\n",
    "            tracking_data = tracking_data.permute(1, 0, 2, 3, 4)     \n",
    "           \n",
    "            labels = labels.view(-1)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames, tracking_data)\n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            retrain_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_retrain = torch.sigmoid(outputs.view(-1)) > 0.5\n",
    "            total_retrain += labels.size(0)\n",
    "            correct_retrain += (predicted_retrain == labels).sum().item()\n",
    "            \n",
    "            # Collect labels and predictions for AUC and other metrics\n",
    "            all_retrain_labels.append(labels.cpu().numpy())\n",
    "            all_retrain_preds.append(outputs.view(-1).cpu().detach().numpy())\n",
    "\n",
    "        retrain_loss /= len(full_train_loader)\n",
    "        retrain_accuracy = 100 * correct_retrain / total_retrain\n",
    "        #train_auc = roc_auc_score(np.concatenate(all_train_labels), np.concatenate(all_train_preds))\n",
    "        retrain_precision = precision_score(np.concatenate(all_retrain_labels), np.concatenate(all_retrain_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        retrain_recall = recall_score(np.concatenate(all_retrain_labels), np.concatenate(all_retrain_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        retrain_f1 = f1_score(np.concatenate(all_retrain_labels), np.concatenate(all_retrain_preds) > 0.5,average='weighted',zero_division=1)\n",
    "        \n",
    "        retrain_losses.append(retrain_loss)\n",
    "        retrain_accuracies.append(retrain_accuracy)\n",
    "        #train_aucs.append(train_auc)\n",
    "        retrain_precisions.append(retrain_precision)\n",
    "        retrain_recalls.append(retrain_recall)\n",
    "        retrain_f1s.append(retrain_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {retrain_loss:.6f}, Train Accuracy: {retrain_accuracy:.2f}%, Train Precision: {retrain_precision:.4f}, Train Recall: {retrain_recall:.4f}, Train F1: {retrain_f1:.4f}')\n",
    "\n",
    "              \n",
    "        \n",
    "    return model, retrain_losses, retrain_accuracies, retrain_precisions, retrain_recalls, retrain_f1s, \n",
    "\n",
    "# Timing retraining \n",
    "retraining_start_time = time.time()\n",
    "model, retrain_losses, retrain_accuracies, retrain_precisions, retrain_recalls, retrain_f1s = retrain_model(model, full_train_loader, criterion, optimizer, scheduler, num_epochs=20, patience=1)\n",
    "retraining_end_time = time.time()\n",
    "retraining_duration = (retraining_end_time - retraining_start_time)/60\n",
    "print(f\"Retraining Duration: {retraining_duration:.2f} minutes\")\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot retraining loss\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(np.arange(1, len(retrain_losses) + 1), retrain_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Retraining Loss per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot retraining accuracy\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(np.arange(1, len(retrain_accuracies) + 1), retrain_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Retraining Accuracy per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Plot retraining Precision\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(np.arange(1, len(retrain_precisions) + 1), retrain_precisions, label='Retraining Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Retraining Precision per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot retraining Recall\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(np.arange(1, len(retrain_recalls) + 1), retrain_recalls, label='Retraining Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Retraining Recall per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot retraining F1 Score\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(np.arange(1, len(retrain_f1s) + 1), retrain_f1s, label='Training F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Retraining F1 Score per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d16127-64f7-4d00-b993-ffd5d65b3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model_res50_late.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb405c1-568e-4a1f-a206-8753c618d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average Retrain Loss: {np.mean(retrain_losses)}')\n",
    "print(f'Average Rerain Accuracy: {np.mean(retrain_accuracies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e489816-a0ac-461d-a60e-bef577d2eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, optimizer, scheduler, device):\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    test_precisions = []\n",
    "    test_recalls = []\n",
    "    test_f1s = []\n",
    "    test_aucs = []\n",
    "    all_test_labels = []\n",
    "    all_test_preds = []\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    pos_weight = torch.tensor([10.0]).to(device)\n",
    "    criterion_with_pos_weight = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, tracking_data, labels in test_loader:            \n",
    "            frames, tracking_data, labels = frames.to(device), tracking_data.to(device), labels.to(device)\n",
    "            frames, tracking_data, labels = frames.float(), tracking_data.float(), labels.float()\n",
    "\n",
    "            # Flatten the first two dimensions (batch, frames)\n",
    "            frames = frames.view(-1, frames.shape[1], frames.shape[2], frames.shape[3], frames.shape[4])\n",
    "            frames = frames.permute(1, 0, 2, 3, 4)            \n",
    "            tracking_data = tracking_data.view(-1, tracking_data.shape[1], tracking_data.shape[2], tracking_data.shape[3], tracking_data.shape[4])\n",
    "            tracking_data = tracking_data.permute(1, 0, 2, 3, 4)          \n",
    "            labels = labels.view(-1)  # Flatten labels to shape (batch*frames)                                             \n",
    "            outputs = model(frames, tracking_data)\n",
    "            \n",
    "            loss = criterion_with_pos_weight(outputs.view(-1), labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_test = torch.sigmoid(outputs.view(-1)) > 0.5\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted_test == labels).sum().item()\n",
    "                        \n",
    "            # Collect labels and predictions for AUC and other metrics\n",
    "            all_test_labels.append(labels.cpu().numpy())\n",
    "            all_test_preds.append(outputs.view(-1).cpu().detach().numpy())\n",
    "            test_loss /= len(test_loader)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracy = 100 * correct_test / total_test\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            \n",
    "            # Calculate precision, recall, f1 for the current batch\n",
    "            test_precision = precision_score(\n",
    "                np.concatenate(all_test_labels),\n",
    "                np.concatenate(all_test_preds) > 0.5,\n",
    "                average='weighted',\n",
    "                zero_division=1\n",
    "            )\n",
    "            test_recall = recall_score(\n",
    "                np.concatenate(all_test_labels),\n",
    "                np.concatenate(all_test_preds) > 0.5,\n",
    "                average='weighted',\n",
    "                zero_division=1\n",
    "            )\n",
    "            test_f1 = f1_score(\n",
    "                np.concatenate(all_test_labels),\n",
    "                np.concatenate(all_test_preds) > 0.5,\n",
    "                average='weighted',\n",
    "                zero_division=1\n",
    "            )\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                test_auc = roc_auc_score(all_test_labels, all_test_preds)\n",
    "            except ValueError:\n",
    "                test_auc = 1.0\n",
    "            test_precisions.append(test_precision)\n",
    "            test_recalls.append(test_recall)\n",
    "            test_f1s.append(test_f1)\n",
    "            test_aucs.append(test_auc)\n",
    "            \n",
    "            # Print scores per batch\n",
    "            print(f'Batch Loss: {test_loss:.4f}, Batch Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}')\n",
    "    \n",
    "    \n",
    "    test_avg_loss = np.mean(test_losses)\n",
    "    test_avg_accuracy = np.mean(test_accuracies)\n",
    "    test_avg_precision = np.mean(test_precisions)\n",
    "    test_avg_recall = np.mean(test_recalls)\n",
    "    test_avg_f1 = np.mean(test_f1s)\n",
    "    test_avg_auc = np.mean(test_aucs)  \n",
    "    # Update scheduler based on average test loss\n",
    "    scheduler.step(test_avg_loss)\n",
    "\n",
    "    return test_avg_loss, test_avg_accuracy, test_avg_precision, test_avg_recall, test_avg_f1, test_avg_auc, test_losses, test_accuracies, test_precisions, test_recalls, test_f1s, test_aucs\n",
    "\n",
    "\n",
    "# Timing training / validation\n",
    "test_start_time = time.time()\n",
    "test_avg_loss, test_avg_accuracy, test_avg_precision, test_avg_recall, test_avg_f1, test_avg_auc, test_losses, test_accuracies, test_precisions, test_recalls, test_f1s, test_aucs = test(model, test_loader, criterion, optimizer, scheduler, device)\n",
    "test_end_time = time.time()\n",
    "test_duration = (test_end_time - test_start_time) / 60\n",
    "print(f\"Testing Duration: {test_duration:.2f} minutes\")\n",
    "print(f\"Average Loss: {test_avg_loss:.4f}, Average Accuracy: {test_avg_accuracy:.4f}\")\n",
    "print(f'Average Precision: {test_avg_precision:.4f}, Average Recall: {test_avg_recall:.4f}, Average F1: {test_avg_f1:.4f}, Average AUC: {test_avg_auc:.4f}')\n",
    "\n",
    "\n",
    "# Plot loss, accuracy, precision, recall, f1, and AUC\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "plt.subplot(3, 2, 1)  # 3 rows, 2 columns, plot 1\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss per Batch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 2)  # 3 rows, 2 columns, plot 2\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy per Batch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 3)  # 3 rows, 2 columns, plot 3\n",
    "plt.plot(test_precisions, label='Test Precision')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Precision (%)')\n",
    "plt.title('Test Precision per Batch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 4)  # 3 rows, 2 columns, plot 4\n",
    "plt.plot(test_recalls, label='Test Recall')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Recall (%)')\n",
    "plt.title('Test Recall per Batch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 5)  # 3 rows, 2 columns, plot 5\n",
    "plt.plot(test_f1s, label='Test F1')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('F1 Score (%)')\n",
    "plt.title('Test F1 per Batch')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 6: Test AUC\n",
    "plt.subplot(3, 2, 6)  # 3 rows, 2 columns, plot 6\n",
    "plt.plot(test_aucs, label='Test AUC')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Test AUC per Batch')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8865246-b893-4f28-b6f5-0e84343dc4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2281dd-3bcc-43f9-b6c4-9ffe1a590452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20119cd9-3f47-4c32-a2aa-c931547e6029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch2.2]",
   "language": "python",
   "name": "conda-env-torch2.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
